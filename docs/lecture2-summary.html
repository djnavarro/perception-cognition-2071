<!DOCTYPE html>
<html>

<head>

<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<title>lecture2-summary</title>


<style type="text/css">
body {
  font-family: Helvetica, arial, sans-serif;
  font-size: 14px;
  line-height: 1.6;
  padding-top: 10px;
  padding-bottom: 10px;
  background-color: white;
  padding: 30px; }

body > *:first-child {
  margin-top: 0 !important; }
body > *:last-child {
  margin-bottom: 0 !important; }

a {
  color: #4183C4; }
a.absent {
  color: #cc0000; }
a.anchor {
  display: block;
  padding-left: 30px;
  margin-left: -30px;
  cursor: pointer;
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0; }

h1, h2, h3, h4, h5, h6 {
  margin: 20px 0 10px;
  padding: 0;
  font-weight: bold;
  -webkit-font-smoothing: antialiased;
  cursor: text;
  position: relative; }

h1:hover a.anchor, h2:hover a.anchor, h3:hover a.anchor, h4:hover a.anchor, h5:hover a.anchor, h6:hover a.anchor {
  background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA09pVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMy1jMDExIDY2LjE0NTY2MSwgMjAxMi8wMi8wNi0xNDo1NjoyNyAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvIiB4bWxuczp4bXBNTT0iaHR0cDovL25zLmFkb2JlLmNvbS94YXAvMS4wL21tLyIgeG1sbnM6c3RSZWY9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9zVHlwZS9SZXNvdXJjZVJlZiMiIHhtcDpDcmVhdG9yVG9vbD0iQWRvYmUgUGhvdG9zaG9wIENTNiAoMTMuMCAyMDEyMDMwNS5tLjQxNSAyMDEyLzAzLzA1OjIxOjAwOjAwKSAgKE1hY2ludG9zaCkiIHhtcE1NOkluc3RhbmNlSUQ9InhtcC5paWQ6OUM2NjlDQjI4ODBGMTFFMTg1ODlEODNERDJBRjUwQTQiIHhtcE1NOkRvY3VtZW50SUQ9InhtcC5kaWQ6OUM2NjlDQjM4ODBGMTFFMTg1ODlEODNERDJBRjUwQTQiPiA8eG1wTU06RGVyaXZlZEZyb20gc3RSZWY6aW5zdGFuY2VJRD0ieG1wLmlpZDo5QzY2OUNCMDg4MEYxMUUxODU4OUQ4M0REMkFGNTBBNCIgc3RSZWY6ZG9jdW1lbnRJRD0ieG1wLmRpZDo5QzY2OUNCMTg4MEYxMUUxODU4OUQ4M0REMkFGNTBBNCIvPiA8L3JkZjpEZXNjcmlwdGlvbj4gPC9yZGY6UkRGPiA8L3g6eG1wbWV0YT4gPD94cGFja2V0IGVuZD0iciI/PsQhXeAAAABfSURBVHjaYvz//z8DJYCRUgMYQAbAMBQIAvEqkBQWXI6sHqwHiwG70TTBxGaiWwjCTGgOUgJiF1J8wMRAIUA34B4Q76HUBelAfJYSA0CuMIEaRP8wGIkGMA54bgQIMACAmkXJi0hKJQAAAABJRU5ErkJggg==) no-repeat 10px center;
  text-decoration: none; }

h1 tt, h1 code {
  font-size: inherit; }

h2 tt, h2 code {
  font-size: inherit; }

h3 tt, h3 code {
  font-size: inherit; }

h4 tt, h4 code {
  font-size: inherit; }

h5 tt, h5 code {
  font-size: inherit; }

h6 tt, h6 code {
  font-size: inherit; }

h1 {
  font-size: 28px;
  color: black; }

h2 {
  font-size: 24px;
  border-bottom: 1px solid #cccccc;
  color: black; }

h3 {
  font-size: 18px; }

h4 {
  font-size: 16px; }

h5 {
  font-size: 14px; }

h6 {
  color: #777777;
  font-size: 14px; }

p, blockquote, ul, ol, dl, li, table, pre {
  margin: 15px 0; }

hr {
  background: transparent url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAYAAAAECAYAAACtBE5DAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAAyJpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvIiB4bWxuczp4bXBNTT0iaHR0cDovL25zLmFkb2JlLmNvbS94YXAvMS4wL21tLyIgeG1sbnM6c3RSZWY9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9zVHlwZS9SZXNvdXJjZVJlZiMiIHhtcDpDcmVhdG9yVG9vbD0iQWRvYmUgUGhvdG9zaG9wIENTNSBNYWNpbnRvc2giIHhtcE1NOkluc3RhbmNlSUQ9InhtcC5paWQ6OENDRjNBN0E2NTZBMTFFMEI3QjRBODM4NzJDMjlGNDgiIHhtcE1NOkRvY3VtZW50SUQ9InhtcC5kaWQ6OENDRjNBN0I2NTZBMTFFMEI3QjRBODM4NzJDMjlGNDgiPiA8eG1wTU06RGVyaXZlZEZyb20gc3RSZWY6aW5zdGFuY2VJRD0ieG1wLmlpZDo4Q0NGM0E3ODY1NkExMUUwQjdCNEE4Mzg3MkMyOUY0OCIgc3RSZWY6ZG9jdW1lbnRJRD0ieG1wLmRpZDo4Q0NGM0E3OTY1NkExMUUwQjdCNEE4Mzg3MkMyOUY0OCIvPiA8L3JkZjpEZXNjcmlwdGlvbj4gPC9yZGY6UkRGPiA8L3g6eG1wbWV0YT4gPD94cGFja2V0IGVuZD0iciI/PqqezsUAAAAfSURBVHjaYmRABcYwBiM2QSA4y4hNEKYDQxAEAAIMAHNGAzhkPOlYAAAAAElFTkSuQmCC) repeat-x 0 0;
  border: 0 none;
  color: #cccccc;
  height: 4px;
  padding: 0;
}

body > h2:first-child {
  margin-top: 0;
  padding-top: 0; }
body > h1:first-child {
  margin-top: 0;
  padding-top: 0; }
  body > h1:first-child + h2 {
    margin-top: 0;
    padding-top: 0; }
body > h3:first-child, body > h4:first-child, body > h5:first-child, body > h6:first-child {
  margin-top: 0;
  padding-top: 0; }

a:first-child h1, a:first-child h2, a:first-child h3, a:first-child h4, a:first-child h5, a:first-child h6 {
  margin-top: 0;
  padding-top: 0; }

h1 p, h2 p, h3 p, h4 p, h5 p, h6 p {
  margin-top: 0; }

li p.first {
  display: inline-block; }
li {
  margin: 0; }
ul, ol {
  padding-left: 30px; }

ul :first-child, ol :first-child {
  margin-top: 0; }

dl {
  padding: 0; }
  dl dt {
    font-size: 14px;
    font-weight: bold;
    font-style: italic;
    padding: 0;
    margin: 15px 0 5px; }
    dl dt:first-child {
      padding: 0; }
    dl dt > :first-child {
      margin-top: 0; }
    dl dt > :last-child {
      margin-bottom: 0; }
  dl dd {
    margin: 0 0 15px;
    padding: 0 15px; }
    dl dd > :first-child {
      margin-top: 0; }
    dl dd > :last-child {
      margin-bottom: 0; }

blockquote {
  border-left: 4px solid #dddddd;
  padding: 0 15px;
  color: #777777; }
  blockquote > :first-child {
    margin-top: 0; }
  blockquote > :last-child {
    margin-bottom: 0; }

table {
  padding: 0;border-collapse: collapse; }
  table tr {
    border-top: 1px solid #cccccc;
    background-color: white;
    margin: 0;
    padding: 0; }
    table tr:nth-child(2n) {
      background-color: #f8f8f8; }
    table tr th {
      font-weight: bold;
      border: 1px solid #cccccc;
      margin: 0;
      padding: 6px 13px; }
    table tr td {
      border: 1px solid #cccccc;
      margin: 0;
      padding: 6px 13px; }
    table tr th :first-child, table tr td :first-child {
      margin-top: 0; }
    table tr th :last-child, table tr td :last-child {
      margin-bottom: 0; }

img {
  max-width: 100%; }

span.frame {
  display: block;
  overflow: hidden; }
  span.frame > span {
    border: 1px solid #dddddd;
    display: block;
    float: left;
    overflow: hidden;
    margin: 13px 0 0;
    padding: 7px;
    width: auto; }
  span.frame span img {
    display: block;
    float: left; }
  span.frame span span {
    clear: both;
    color: #333333;
    display: block;
    padding: 5px 0 0; }
span.align-center {
  display: block;
  overflow: hidden;
  clear: both; }
  span.align-center > span {
    display: block;
    overflow: hidden;
    margin: 13px auto 0;
    text-align: center; }
  span.align-center span img {
    margin: 0 auto;
    text-align: center; }
span.align-right {
  display: block;
  overflow: hidden;
  clear: both; }
  span.align-right > span {
    display: block;
    overflow: hidden;
    margin: 13px 0 0;
    text-align: right; }
  span.align-right span img {
    margin: 0;
    text-align: right; }
span.float-left {
  display: block;
  margin-right: 13px;
  overflow: hidden;
  float: left; }
  span.float-left span {
    margin: 13px 0 0; }
span.float-right {
  display: block;
  margin-left: 13px;
  overflow: hidden;
  float: right; }
  span.float-right > span {
    display: block;
    overflow: hidden;
    margin: 13px auto 0;
    text-align: right; }

code, tt {
  margin: 0 2px;
  padding: 0 5px;
  white-space: nowrap;
  border: 1px solid #eaeaea;
  background-color: #f8f8f8;
  border-radius: 3px; }

pre code {
  margin: 0;
  padding: 0;
  white-space: pre;
  border: none;
  background: transparent; }

.highlight pre {
  background-color: #f8f8f8;
  border: 1px solid #cccccc;
  font-size: 13px;
  line-height: 19px;
  overflow: auto;
  padding: 6px 10px;
  border-radius: 3px; }

pre {
  background-color: #f8f8f8;
  border: 1px solid #cccccc;
  font-size: 13px;
  line-height: 19px;
  overflow: auto;
  padding: 6px 10px;
  border-radius: 3px; }
  pre code, pre tt {
    background-color: transparent;
    border: none; }

sup {
    font-size: 0.83em;
    vertical-align: super;
    line-height: 0;
}

kbd {
  display: inline-block;
  padding: 3px 5px;
  font-size: 11px;
  line-height: 10px;
  color: #555;
  vertical-align: middle;
  background-color: #fcfcfc;
  border: solid 1px #ccc;
  border-bottom-color: #bbb;
  border-radius: 3px;
  box-shadow: inset 0 -1px 0 #bbb
}

* {
	-webkit-print-color-adjust: exact;
}
@media screen and (min-width: 914px) {
    body {
        width: 854px;
        margin:0 auto;
    }
}
@media print {
	table, pre {
		page-break-inside: avoid;
	}
	pre {
		word-wrap: break-word;
	}
}
</style>


</head>

<body>

<h1 id="toc_0">Lecture 2: Attention</h1>

<p><em>[Disclaimer: These informal lecture notes are not intended to be comprehensive - there are some additional ideas in the lectures and lecture slides, textbook, tutorial materials etc. As always, the lectures themselves are the best guide for what is and is not examinable content. However, I hope they are useful in picking out the core content in each lecture.]</em></p>

<h2 id="toc_1">Part 1: Definitions</h2>

<p>The quote by William James at the start of the lecture is used to motivate three key questions. Here it is again in full:</p>

<blockquote>Attention isâ€¦the taking into possession of the mind, in clear and vivid form, of one out of what seem several simultaneously possible objects or trains of thought.</blockquote>

<p>The three questions posed in the lecture are:</p>

<ul>
<li><em>How many</em> things are we attending to? (&quot;one&quot; or &quot;several&quot;)</li>
<li><em>What kind</em> of things are we paying attention to (&quot;objects&quot; or &quot;thoughts&quot;)</li>
<li><em>What controls</em> our attention? Do we direct it or does the world &quot;take possession&quot;?</li>
</ul>

<h3 id="toc_2">List of different distinctions</h3>

<p>From these three questions, we obtain several quite distinct aspects to attention. By considering the &quot;how many&quot; question, we arrive at the distinction between:</p>

<ul>
<li><strong>Focused attention</strong> (a.k.a. selective attention)... when the goal is to attend to one thing and ignore all the other things</li>
<li><strong>Divided attention</strong> (a.k.a. multitasking)... when the goal is to keep track of many things </li>
</ul>

<p>The &quot;what kind&quot; question leads us to consider the difference between</p>

<ul>
<li><strong>External attention</strong>... when the <em>focus</em> of attention is something in the world.</li>
<li><strong>Internal attention</strong>... when the focus of attention is something you&#39;re thinking about</li>
</ul>

<p>External attention is usually subdivided by sensory modality: <strong>visual attention</strong> is when you&#39;re paying attention to something you can see, <strong>auditory attention</strong> is directed at things you can hear, and <strong>cross-modal attention</strong> occurs when attention is directed to multiple senses at once (e.g., to the smell and taste of a cup of coffee) </p>

<p>Finally, the &quot;what controls&quot; question leads us to consider</p>

<ul>
<li><strong>Active attention</strong> (endogenous control)... is when we exercise &quot;top down&quot; control to achieve our goals</li>
<li><strong>Passive attention</strong> (exogenous control)... is when something in the world imposes &quot;bottom up&quot; control, demanding our attention</li>
</ul>

<h3 id="toc_3">Examples</h3>

<p>This list of attentional &quot;types&quot; provides a pretty effective classification system for attentional phenomena. Consider the following examples:</p>

<blockquote>A car backfires in my street, causing me to startle and look out the window</blockquote>

<p>The attention here is:</p>

<ul>
<li><em>passive</em>, because it is being controlled by the world</li>
<li><em>external</em>, because it is directed at something in the world (specifically, something <em>auditory</em>)</li>
<li><em>focused</em>, because it is directed at a single thing</li>
</ul>

<h2 id="toc_4">Part 2: Phenomena in auditory attention</h2>

<p>A lot of the early work in attention focused on auditory attention, apparently because that was easier to study with the technology of the time. A lot of this work is inspired by the so-called <strong>cocktail party problem</strong> - trying to pay attention to a single &quot;source&quot; (e.g., one person speaking) when there are many different sources competing for your attention (e.g., it&#39;s a very noisy room and there are many people talking at once)</p>

<p>Much of this research relied on a paradigm called the <strong>dichotic listening task</strong>, in which a participant wears headphones and two different auditory signals are presented, one to the left ear and the other to the right ear. The task is to <em>shadow</em> the content in one ear (e.g., right ear), by repeating aloud what you hear in the target (right) ear, and ignore the content in the non-target ear (left).</p>

<p><strong>[Empirical result]</strong> A typical pattern of results (e.g., Moray 1959) is that people are very good at the task, shadowing the target signal successfully. However, when tested, they remember very little of the content presented in the non-target ear. They can perhaps recall whether it was a male or female voice, and a few other &quot;low level&quot; perceptual features, but semantic information (i.e., the content!) seems not to be recalled.</p>

<p><strong>[Theoretical idea]</strong> These kinds of findings led Broadbent (1954) to propose an <strong>early selection</strong> view of attention, sometimes referred to as &quot;Broadbent&#39;s filter&quot;. The idea is that the perceptual system does some &quot;simple&quot; perceptual analysis of every signal, detecting things like pitch, loudness, frequency, etc. On the basis of this <em>preattentive analysis</em> the filter selects a single signal to pay attention to. Only this selected signal is processed further, and as a consequence we are only aware of the semantic information (i.e., meaningful content) within the attended stream. This idea explains the basic empirical results described earlier, but it has some problems.</p>

<p>Early selection theory predicts that we use low-level perceptual features to select the target for attention. Yet there is evidence that people use semantic information to do so. It also predicts that we don&#39;t process the semantic content of the unattended stream, yet that also seems not to be correct. </p>

<p><strong>[Empirical result]</strong> Here&#39;s the relevant findings from the lecture:</p>

<ul>
<li><p>Gray &amp; Wedderburn (1960) - when the semantic content switches from one ear to the other, people follow the switch, preferring to shadow the semantic content. This makes a lot of intuitive sense, but it is inconsistent with early selection theory because we shouldn&#39;t be able to do this if the semantic processing only occurs <em>after</em> attention has been allocated.</p></li>
<li><p>Lewis (1970) - when the unattended stream has contains information that is very semantically similar to the attended stream (e.g., two words with the same meaning but very different sounds), the unattended stream interferes with the shadowing task. This is known as <strong>semantic interference</strong></p></li>
<li><p>And of course, this was actually sort of well known from the beginning. Even Moray (1959) reported the fact that there are some special stimuli (e.g., your own name) that you will automatically (i.e., passively) attend to even if they appear in an unattended stream. To the extent that this is a <em>semantic</em> phenomenon (i.e., it occurs because your name is meaningful to you), this is an example of something where semantic content plays a role in the allocation of attention</p></li>
</ul>

<p><strong>[Theoretical interpretation]</strong> There are two alternative theories that were discussed that are able to account for these findings.</p>

<ul>
<li><strong>late selection</strong> (Deutsch &amp; Deutsch 1963) claims that all signals are processed almost entirely, including all the semantic information, it&#39;s just that we&#39;re often unaware of this. The claim is that because working memory capacity is limited, we&#39;re only able to &quot;hold&quot; a small amount of information in mind at once, so we&#39;re only &quot;aware&quot; of the attended stream. </li>
<li><strong>attenuation theory</strong> (Treisman 1964) proposes that sometimes we use early selection and sometimes we use late selection. Specifically, the claim is that we process every signal up until the point that we are able to determine whether it&#39;s relevant (i.e., is this the target signal I&#39;m trying to attend to?). As soon as it becomes clear that this is not the target signal, processing stops</li>
</ul>

<p>In the empirical literature there are follow up studies that seek to distinguish between these possibilities, but they&#39;re beyond the scope of this lecture.</p>

<h2 id="toc_5">Part 3: Phenomena in visual attention</h2>

<p>In the third part of the lecture we asked whether the phenomena listed above are idiosyncratic to audition, or whether they are general phenomena that could be found in any sensory modality. We discussed two studies in particular.</p>

<ul>
<li><p>Rock &amp; Guttman (1981) devised a visual analog of the shadowing task, in which people were shown two shapes drawn over the top of one another, one red and the other green, and the task is to rate the aesthetic appeal of the target &quot;stream&quot; (e.g., the red shapes) while ignoring the other stream (e.g., green). Participants were presented with many of these trials in quick succession. At the end, they were tested on their memory for whether they&#39;d seen various shapes. Mirroring the typical results in auditory tasks, people had very good memory for shapes that appeared in the attended stream (red) and very poor memory for those in the unattended stream (green)</p></li>
<li><p>Tipper (1985) pushed this analogy further, demonstrating a <strong>negative priming</strong> effect in visual attention that very closely mirrors the semantic interference effect in auditory attention (see above). The methodology was very similar to Rock &amp; Guttman, but used illustrations of familiar, meaningful objects. The key result is that when the previous item in the unattended stream (e.g., a green wolf) is semantically related to the current item in the attended stream (e.g., a red cat), people are slower to respond to the current item. What this suggests is tha semantic information in the unattended (green) stream is in fact being processed. Whether we realise it or not, we are detecting that there is a wolf in the green stream and are &quot;suppressing&quot; our response to it because it belongs to the unattended stream. However, what this does is suppress our willingness to respond to wolves, cats, dogs, etc; so we are slightly slower to respond when the next item in the attended stream happens to be a red cat. </p></li>
</ul>

<p>The key take home message from this is that there are some remarkable similarities in how attention operates across different sensory modalities. </p>

<h2 id="toc_6">Part 4: Visual search</h2>

<p>The final part of the lecture discussed &quot;visual search&quot; tasks, in which the goal is to find a &quot;target object&quot; hidden among a collection of distractors (e.g., find my child among the crowd of children in the playground)</p>

<p><strong>[empirical results]</strong> When the target is defined by a specific feature (e.g., colour) it seems to <strong>&quot;pop out&quot;</strong>. Attention is automatically (i.e., passively) drawn to the target item. The <strong>set size</strong> (i.e., number of distractor items) makes no difference to the search time. This phenomenon is quite general, and doesn&#39;t depend on what kind of feature differentiates the target item: colour, shape, size, orientation, motion, depth, all produce pop out effects.</p>

<p>In contrast, when the target does not possess any unique features, there is no pop-out effect. For instance, if we need to find a red horizontal rectangle in a field of red vertical rectangles and blue horizontal rectangles, you need to make use of <em>both</em> features (i.e., orientation and colour) to solve the search problem. Search is slower, and now the set size matters: the more distractors there are, the slower you are to find the target. </p>

<p><strong>[theoretical interpretation]</strong>    The explanation for this proposed by Treisman (1986) is referred to as &quot;feature integration theory&quot;. The idea is that the perceptual system has many different &quot;feature analyzers&quot; that detect perceptual features (e.g., red, blue, horizontalness etc). These operate quickly and in parallel - so if you can solve a visual search problem using only a single feature, then set size is irrelevant because the <em>parallel</em> nature of the feature analyzers means that you&#39;re processing every part of the visual input at the same time. </p>

<p>However, the feature analyzers are distinct from one another. Just because one analyzer has detected &quot;redness&quot; at a particular location and another has detected &quot;horizontalness&quot; at the same location doesn&#39;t mean that we automatically <em>bind</em> those two pieces of information together into a unified representation of the object. In order to do this <strong>feature binding</strong>, we need to direct <em>attention</em> to the location in question. Because attention is a slow, serial process (i.e., does one thing at a time), any visual search problem that requires feature binding (i.e., we need to use multiple features to solve it) will not produce a pop out effect, and visual search time will be slower when the set size is larger</p>

<p><strong>[empirical data]</strong> One piece of evidence for feature integration theory (FIT) is the <strong>illusory conjunction</strong> phenomenon. FIT predicts that Ffature extraction occurs automatically and in parallel; but object recognition requires feature binding, a process that requires slow serial attentional processing of stimuli to be done accurately. If this is not allowed (e.g., stimuli are presented too quickly for attention to come into play), then errors in binding will occur and will be based on features extracted automatically during early perceptual processing. See lecture slides for illustration.</p>




</body>

</html>
